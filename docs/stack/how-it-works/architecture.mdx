---
title: Architecture
description: Architecture of the Linea stack components and their interactions
sidebar_position: 2
mainSource:
  url: 'https://github.com/Consensys/linea-monorepo'
image: /img/socialCards/architecture.jpg
---

import DocCardList from '@theme/DocCardList';

export const relatedDocs = [
  {type: 'link', href: '/protocol/architecture', label: 'Protocol Architecture'},
  {type: 'link', href: '/protocol/transaction-lifecycle', label: 'Public Mainnet transaction lifecycle'},
  {type: 'link', href: '/stack/how-it-works/deployment-models', label: 'Deployment Models'},
  {type: 'link', href: '/stack/how-it-works/boundaries-responsibilities', label: 'Boundaries & Responsibilities'},
  {type: 'link', href: '/stack/how-it-works/smart-contracts', label: 'Smart contracts'},
];

As an EVM-compatible system, the Linea stack uses a dual-layer architecture with separate consensus and 
execution layers. This design enables modular deployment, client diversity, and operational flexibility.

:::tip

This page describes the **operator view** of the Linea stack: what components run where, how they
connect, and which responsibilities fall on the network operator. For protocol-level mechanics see the
[protocol architecture](/protocol/architecture).

::: 

## Dual-layer architecture

The Linea stack separates consensus from execution:

- Consensus layer: Responsible for block production, validator coordination, and block propagation.
- Execution layer: Responsible for executing transactions, maintaining EVM state, and serving RPC requests.

The two layers communicate via the [Engine API](#full-nodes), a standard Ethereum interface that supports the clean 
separation of consensus from execution. This allows operators to change execution clients or scale 
components without restructuring the entire network.

## What runs where

From an operational perspective, the Linea stack is composed of three broad classes of components.

### Node roles

In a fully-public network, these components run on externally reachable nodes, other deployment types 
may [assume different strategies](#network-topology-patterns).

- Validator nodes  
  Participate in QBFT consensus. Validator nodes run a consensus client and an execution client.

- Full nodes  
  Maintain full chain state, verify blocks, and serve RPC requests. Full nodes do not necessarily
  participate in consensus.

- RPC nodes  
  Expose JSON-RPC APIs to applications and users. RPC nodes may be optimized for near-head access or
  configured as archive nodes for historical queries.

### Internal protocol services

These services are required for proof generation and finalization but are **not** exposed publicly.

- Sequencer
- Coordinator
- Prover
- State manager
- Tracer

Internal services typically communicate over operator-controlled interfaces--similar to the [Engine API](#full-nodes) used between consensus and execution clients. Traffic between these services remains within the operatorâ€™s trusted network boundary and does not traverse public networks.

## Core components

The Linea stack consists of the following core components. Each component has a clear operational
responsibility and deployment footprint.

### Maru

[Maru](../../protocol/repos.mdx#maru), the consensus client runs on validator nodes and coordinates 
block production and consensus participation. 

<details>
<summary>Further details</summary>

Maru:
  - Coordinates block production with the execution layer
  - Manages validator sets and consensus participation
  - Provides P2P networking for block propagation
  - Exposes API endpoints for monitoring and control

Maru requires a minimum of 4 nodes for QBFT consensus to ensure fault tolerance (3f+1 where f is the
number of faulty nodes). 

</details>

### Linea Besu

The execution layer client, [Linea Besu](../../protocol/repos.mdx#linea-besu-upstream) executes 
transactions and maintains EVM state. 

### Sequencer

The [sequencer](../../protocol/architecture/sequencer/index.mdx) orders transactions and builds blocks according to Linea rules. 
Typically deployed as a protected internal service.  

:::info

Because the Besu Execution Layer (EL) client is fully extensible with plugins, this allow operators to extend and apply arbitrary business logic such as enforcing regulatory logic at the block building level, controlling which transactions may be executed by the EVM.

:::

### Coordinator

The [coordinator](../../protocol/architecture/coordinator.mdx) orchestrates batching, proof generation, and 
submission to the finalization layer.  

### Prover

The [prover](../../protocol/architecture/prover/index.mdx) generates zero-knowledge proofs for state transitions. Provers may be 
scaled horizontally to meet throughput requirements.  


### State manager

The [state manager](../../protocol/architecture/state-manager.mdx) maintains a state representation optimized for 
proof generation and recovery.  

### Tracer 

The [tracer](../../protocol/repos.mdx#linea-tracer) generates execution traces required for proof generation.  

<details>
<summary>Further details</summary>

The tracer is a Linea Besu plugin that:

- Generates execution traces during block processing
- Provides trace counts for batch size calculations
- Creates conflated trace files for prover input

Trace data is essential for proof generation and must be available for all blocks in a batch.

</details>

## Transaction lifecycle

These architectural components support transactions from submission to
finalization. 

1. Submission: Transactions enter the mempool via RPC entrypoint of a [full node](#full-nodes).
2. Block building: [sequencer](../../protocol/architecture/sequencer/index.mdx) orders and executes transactions into blocks.
3. State tracking: [state manager](../../protocol/architecture/state-manager.mdx) updates state representation;
   [tracer](../../protocol/repos.mdx#linea-tracer) generates traces.
4. Conflation: [coordinator](../../protocol/architecture/coordinator.mdx) groups blocks into batches.
5. Proof generation: [prover](../../protocol/architecture/prover/index.mdx) generates execution proofs for batches.
6. Blob creation: [coordinator](../../protocol/architecture/coordinator.mdx) combines batches into blobs; 
[prover](../../protocol/architecture/prover/index.mdx) generates compression proofs.
7. Finalization: [coordinator](../../protocol/architecture/coordinator.mdx) submits proofs to finalization 
layer; after verification and epoch delay, transactions reach hard finality.

:::info

See the [Public Mainnet walkthrough of a transaction lifecycle](../../protocol/transaction-lifecycle.mdx) 
for further details.

::: 

## Onchain system contracts

Smart contracts execute small programs that rely on the EVM's trustless correctness guarantees. Once deployed, they are immutable.

The Linea stack includes system contracts deployed in two locations:

- On the finalization layer: The finalization contract validates [zk-SNARK](../../protocol/zero-knowledge-glossary#zk-snark-zero-knowledge-succinct-non-interactive-argument-of-knowledge) proofs and records finalized state roots. This contract must be deployed on the [finalization layer](./data-availability-finalization.mdx).

- On the Linea network itself: Contracts such as `Settlement` and `TokenFactory` are deployed on your Linea network instance to enable token issuance, settlement workflows, and other network-specific functionality.

- On both layers: Bridge contracts are deployed on both the destination chain and the Linea network to enable cross-chain token transfers and message passing.

For details on all system contracts, see [smart contracts](./smart-contracts.mdx).


## Auxiliary service

### Block explorer

The block explorer is an optional, operator-facing service used for inspection, troubleshooting, and
auditing. Linea deployments commonly use [Blockscout](https://github.com/blockscout/blockscout), an open-source, self-hosted explorer that
indexes chain data from execution nodes and presents it via a web interface.

The explorer connects to the network via JSON-RPC or WebSocket, continuously indexes blocks,
transactions, logs, and token data, and stores this information in a relational database. It allows
operators to inspect blocks and transactions, review contract activity, track bridge operations, and
monitor network health.

Block explorers are not required for network operation and do not participate in consensus,
execution, or finalization. They can be deployed, replaced, or omitted without affecting correctness
or liveliness.

## Network topology

### Full nodes

Full nodes are composed of the execution layer client such as
[Besu](../../network/how-to/run-a-node/linea-besu.mdx#run-using-docker) or [Geth](../../network/how-to/run-a-node/geth.mdx), 
and the consensus layer [Maru](../../network/how-to/run-a-node/maru.mdx). Full nodes maintain complete 
network state and participate in consensus:

- Verify all blocks and transactions
- Serve RPC requests
- Participate in P2P networking
- May serve as validators in QBFT consensus

Full nodes are often run with both clients in the same container. These clients communicate via the Engine API, an internal service, as shown in the diagram below.

<img
  src="/img/stack/full-node.png"
  alt="Full node topology"
  style={{display: 'block', margin: '0 auto', width: 500}}
/>

The block is proposed by the execution layer, Linea Besu, and the consensus layer, Maru signs and broadcasts it.

### RPC nodes

RPC nodes provide API access to the network:

- Near-head nodes: Optimized for low-latency access to recent state
- Archive nodes: Maintain complete historical state for auditing and analysis

RPC nodes may be deployed behind load balancers and access controls (RBAC) for [private networks](./deployment-models/private-validium.mdx).

#### Near-head nodes

A near-head node maintains a replicated view of the blockchain focused on the canonical head and its recent history. They continuously ingest new blocks and state updates from the protocol, ensuring that its local state remains closely synchronized with the latest finalized and near-finalized data. The service is optimized for low-latency access to recent blocks and current state, and for high-throughput transaction propagation toward the rest of the system. It is designed to support latency-sensitive operations on near-tip state, rather than long-range historical reconstruction or analysis.

#### Archive nodes

An archive node maintains a complete, durable record of the blockchain from genesis, including all blocks and associated state transitions. Unlike a near-head node, it retains full historical state, enabling exact reconstruction of chain state at any past block height. This service is optimized for correctness and completeness of historical data rather than minimal latency, and is provisioned with higher storage capacity and different performance characteristics. Its role in the architecture is to provide a source of truth for historical state, supporting downstream functions such as auditing, forensics, analytics, and historical consistency checks.

### Network topology patterns

Network topology varies by [deployment model](./deployment-models/index.mdx):

- [Private networks](./deployment-models/private-validium.mdx)  
  All components run within controlled infrastructure. Data availability and access controls are
  enforced.

- [Hybrid networks](./deployment-models#hybrid)  
  Internal services and private data availability coexist with selective public interaction.

- [Public networks](deployment-models#public)  
  Data availability is onchain, and components interact with public infrastructure.

## Data availability

Proofs and associated payloads are submitted to the finalization layer to provide data availability 
guarantees. Your data availability strategy depends on your deployment model:

- Private Validium: Data stored offchain with private node set; no onchain data posting
- Hybrid: Selective data availability based on transaction type
- Public: Data posted to finalization layer using EIP-4844 blobs

For details on data availability and choices around finalization layers, see 
[data availability and finalization](./data-availability-finalization.mdx).

## Related documentation

<DocCardList items={relatedDocs} />
